{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, VarianceThreshold, SequentialFeatureSelector, SelectPercentile, SelectFpr, SelectFdr, SelectFwe, RFE \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import skew\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.patches as mpatches\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from src.emotion.prediction.aggregates.train import HyperparaSearch\n",
    "from src.emotion.prediction.aggregates.models import MODELS\n",
    "from src.emotion.prediction.aggregates.classifier import CLASSIFIER\n",
    "\n",
    "# %matplotlib inline\n",
    "# plt.style.use('ggplot')\n",
    "\n",
    "# # set default color cycle\n",
    "# plt.rcParams['axes.prop_cycle'] = plt.cycler(color=plt.cm.tab10.colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('/home/moritz/Workspace/masterthesis/data/features_dataset_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_csv('/home/moritz/Workspace/masterthesis/data/perma_scores_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 135)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(features, targets, on=[\"E-Mail-Adresse\", \"Day\"])\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the name of the file\n",
    "filename = \"facial_features.csv\"\n",
    "feats = df.drop(columns=['ClassID'])\n",
    "# Save the DataFrame to CSV\n",
    "# Suppose 'Email-Address' and 'B' are the columns you want to move to the front\n",
    "cols_to_move = ['E-Mail-Adresse', 'Day', 'First Name', 'Last Name/Surname']\n",
    "\n",
    "# Get the rest of the columns and concatenate two lists\n",
    "new_order = cols_to_move + [col for col in feats.columns if col not in cols_to_move]\n",
    "\n",
    "# Reorder the DataFrame\n",
    "feats = feats[new_order]\n",
    "\n",
    "feats.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Missing Values\n",
    "\n",
    "df.dropna(axis=1, how='any', inplace=True)\n",
    "#df = dataset.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers\n",
    "# find columns where all values are the same\n",
    "cols_to_drop = [col for col in df.columns if df[col].nunique() == 1]\n",
    "# drop the columns\n",
    "df = df.drop(cols_to_drop, axis=1)\n",
    "print(df.shape)\n",
    "\n",
    "# drop columns where all values are only 0 or 1\n",
    "df = df.loc[:, ~(df.isin([0, 1]).all() & ~df.isin([0, 1]).any())]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load X and Y\n",
    "# Store the PERMA values in Y\n",
    "Y = df[['P', 'E', 'R', 'M', 'A']]\n",
    "\n",
    "# Store the other columns in X\n",
    "X = df.drop(columns=['ClassID', 'E-Mail-Adresse', 'Day', 'First Name', 'Last Name/Surname', 'P', 'E', 'R', 'M', 'A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "# # define the model\n",
    "# print(X.shape)\n",
    "# lof = LocalOutlierFactor(contamination=0.05)\n",
    "# # fit the model\n",
    "# yhat = lof.fit_predict(X)\n",
    "\n",
    "# # select all rows that are not outliers\n",
    "# mask = yhat != -1\n",
    "# X = X[mask]\n",
    "# Y = Y[mask]\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Features\n",
    "# Create a MinMaxScaler object\n",
    "minmax_scaler = MinMaxScaler()\n",
    "gaussian_scaler = StandardScaler()\n",
    "non_gaussian_scaler = RobustScaler()\n",
    "\n",
    "# Calculate skewness of each feature\n",
    "skewness = skew(X, axis=0)\n",
    "\n",
    "# Create empty list to store scaled features\n",
    "scaled_features = []\n",
    "\n",
    "# Scale features using appropriate scaler\n",
    "for feat, s in zip(X.columns, skewness):\n",
    "    if abs(s) < 0.5:\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = RobustScaler()\n",
    "    scaled_feat = scaler.fit_transform(X[feat].values.reshape(-1, 1)).flatten()\n",
    "    scaled_features.append(pd.Series(scaled_feat, name=feat))\n",
    "\n",
    "# Concatenate scaled features into new DataFrame\n",
    "scaled_X = pd.concat(scaled_features, axis=1)\n",
    "\n",
    "\n",
    "# Fit the scaler to the dataframe and transform the dataframe\n",
    "scaled_Y = pd.DataFrame(minmax_scaler.fit_transform(Y), columns=['P', 'E', 'R', 'M', 'A'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(matrix, title):\n",
    "    # center the matrix\n",
    "    matrix = matrix - np.mean(matrix, axis=0)\n",
    "\n",
    "    # transpose the matrix\n",
    "    matrix_t = matrix.T\n",
    "\n",
    "    # compute the correlation matrix using np.corrcoef\n",
    "    corr_matrix = np.corrcoef(matrix_t)\n",
    "\n",
    "    # create a heatmap of the correlation matrix using seaborn\n",
    "    sns.set(font_scale=0.7)\n",
    "    sns.heatmap(corr_matrix, cmap=\"YlGnBu\")\n",
    "    \n",
    "    path = \"/home/moritz/Dropbox (Personal)/Dropbox/Apps/Overleaf/tum-thesis-latex/data/appendix/\"\n",
    "    image = path + title + \".pdf\"\n",
    "    plt.savefig(image, bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(scaled_X, scaled_Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"corr_before\"\n",
    "plot_correlation_matrix(X_train, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate correlation matrix\n",
    "# corr_matrix = X_train.corr().abs()\n",
    "\n",
    "# # Create a DataFrame from the correlation matrix\n",
    "# corr_df = corr_matrix.unstack().reset_index()\n",
    "\n",
    "# # Rename columns\n",
    "# corr_df.columns = ['Feature1', 'Feature2', 'Correlation']\n",
    "\n",
    "# # Remove self correlations by excluding cases where variables are correlated with themselves\n",
    "# corr_df = corr_df[corr_df['Feature1'] != corr_df['Feature2']]\n",
    "\n",
    "# # Drop duplicates\n",
    "# corr_df = corr_df.drop_duplicates()\n",
    "\n",
    "# # Sort by absolute correlation\n",
    "# corr_df['AbsCorrelation'] = np.abs(corr_df['Correlation'])\n",
    "# corr_df.sort_values(by='AbsCorrelation', ascending=False, inplace=True)\n",
    "\n",
    "# # Identify highly correlated feature pairs\n",
    "# high_corr_pairs = []\n",
    "# for index, row in corr_df.iterrows():\n",
    "#     if row['AbsCorrelation'] > 0.9:\n",
    "#         high_corr_pairs.append((row['Feature1'], row['Feature2']))\n",
    "\n",
    "# # From each pair, remove the feature with lower average correlation\n",
    "# to_drop = set()\n",
    "# for pair in high_corr_pairs:\n",
    "#     feature1_avg_corr = corr_df[(corr_df['Feature1'] == pair[0]) | (corr_df['Feature2'] == pair[0])]['AbsCorrelation'].mean()\n",
    "#     feature2_avg_corr = corr_df[(corr_df['Feature1'] == pair[1]) | (corr_df['Feature2'] == pair[1])]['AbsCorrelation'].mean()\n",
    "\n",
    "#     if feature1_avg_corr < feature2_avg_corr:\n",
    "#         to_drop.add(pair[0])\n",
    "#     else:\n",
    "#         to_drop.add(pair[1])\n",
    "\n",
    "# # Drop highly correlated features from original dataframe\n",
    "# for feature in to_drop:\n",
    "#     if feature in X_train.columns:\n",
    "#         X_train = X_train.drop(columns=[feature])\n",
    "\n",
    "# print(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"corr_after\"\n",
    "plot_correlation_matrix(X_train, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_voting_features_combined(Y, X, filter_methods, wrapper_methods, k):\n",
    "    #np.random.seed(1)\n",
    "    \n",
    "    feature_counts = np.zeros(X.shape[1])\n",
    "\n",
    "    # Apply filter methods\n",
    "    for _, method in filter_methods.items():\n",
    "        method.fit(X, Y)\n",
    "        selected_i = method.get_support(indices=True)\n",
    "        print(len(selected_i))\n",
    "        \n",
    "        # Increment the count for each selected feature\n",
    "        for index in selected_i:\n",
    "            feature_counts[index] += 1\n",
    "\n",
    "    # Apply wrapper methods\n",
    "    for _, method in wrapper_methods.items():\n",
    "        method.fit(X, Y)\n",
    "        selected_i = method.get_support(indices=True)\n",
    "        \n",
    "        # Increment the count for each selected feature\n",
    "        for index in selected_i:\n",
    "            feature_counts[index] += 1\n",
    "    \n",
    "    # Get the indices of the top k features with the most counts\n",
    "    #selected_features = np.argsort(feature_counts)[-k:]\n",
    "    selected_features = np.where(feature_counts >= k)[0]\n",
    "    print(np.sort(feature_counts))\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for each PERMA pillar\n",
    "perma_dict = {\n",
    "    \"P\": None,\n",
    "    \"E\": None,\n",
    "    \"R\": None,\n",
    "    \"M\": None,\n",
    "    \"A\": None,\n",
    "}\n",
    "\n",
    "filter_methods = {\n",
    "    'f_regression': SelectKBest(f_regression, k=8),\n",
    "    'mutual_info_regression': SelectKBest(mutual_info_regression, k=8),\n",
    "    'variance_threshold': VarianceThreshold(threshold=1.5),\n",
    "    'select_percentile': SelectPercentile(f_regression, percentile=10),\n",
    "    'select_fpr': SelectFpr(f_regression, alpha=0.15),\n",
    "    'select_fdr': SelectFdr(f_regression, alpha=0.99),\n",
    "    'select_fwe': SelectFwe(f_regression, alpha=1.0),\n",
    "    #'rfe_regression': RFEC(LinearRegression(), n_features_to_select=8),\n",
    "}\n",
    "\n",
    "wrapper_methods = {\n",
    "    #'sfs_regression': SequentialFeatureSelector(LinearRegression(), n_features_to_select=8, direction='forward', n_jobs=-1),\n",
    "    #'sbs_regression': SequentialFeatureSelector(LinearRegression(), n_features_to_select=8, direction='backward', n_jobs=-1),\n",
    "}\n",
    "\n",
    "\n",
    "# Iterate over each PERMA dimension in Y\n",
    "for i, perma_dim in enumerate(Y.columns):\n",
    "    # Get the selected features for the current PERMA dimension\n",
    "    selected_features = get_selected_voting_features_combined(Y_train.iloc[:, i], X_train, filter_methods, wrapper_methods, k=4)\n",
    "    # Save the selected features in the corresponding dictionary for the current PERMA pillar\n",
    "    perma_dict[perma_dim] = selected_features\n",
    "\n",
    "# Print the selected features for each PERMA dimension in each PERMA pillar\n",
    "for pillar in perma_dict:\n",
    "    print(pillar, \":\", perma_dict[pillar])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = {\n",
    "    \"P\": {\"train\": None, \"test\": None},\n",
    "    \"E\": {\"train\": None, \"test\": None},\n",
    "    \"R\": {\"train\": None, \"test\": None},\n",
    "    \"M\": {\"train\": None, \"test\": None},\n",
    "    \"A\": {\"train\": None, \"test\": None}\n",
    "}\n",
    "\n",
    "for perma_dim, selected_features in perma_dict.items():\n",
    "    # Select the corresponding columns of X_filtered\n",
    "    X_train_final = X_train.iloc[:, list(set(selected_features))]\n",
    "    #X_test_final = X_test.iloc[:, list(set(selected_features))]\n",
    "    column_names = X_train_final.columns.tolist()\n",
    "    X_test_final = X_test[column_names]\n",
    "    # Add the selected features for the current PERMA dimension to the feature sets dictionary\n",
    "    feature_sets[perma_dim][\"train\"] = X_train_final\n",
    "    feature_sets[perma_dim][\"test\"] = X_test_final\n",
    "    # Print the names of the selected features\n",
    "    column_names = X_train_final.columns.tolist()\n",
    "    print(perma_dim, \":\", column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    BayesianRidge,\n",
    "    ElasticNet,\n",
    "    Lasso,\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "MODELS = [\n",
    "    {\n",
    "        \"name\": \"MLPRegressor\",\n",
    "        \"model\": MLPRegressor(max_iter=2000),\n",
    "        \"params\": {\n",
    "            \"hidden_layer_sizes\": [(64, 32), (128, 64)],\n",
    "            \"alpha\": [0.001, 0.01, 0.1],\n",
    "            \"learning_rate_init\": [0.001, 0.01, 0.1],\n",
    "            \"random_state\": [random_state],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"KNeighborsRegressor\",\n",
    "        \"model\": KNeighborsRegressor(),\n",
    "        \"params\": {\"n_neighbors\": [1, 5, 10, 15, 20]},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DecisionTreeRegressor\",\n",
    "        \"model\": DecisionTreeRegressor(),\n",
    "        \"params\": {\"max_depth\": range(2, 11)},\n",
    "        \"random_state\": [random_state],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"RandomForestRegressor\",\n",
    "        \"model\": RandomForestRegressor(),\n",
    "        \"params\": {\"n_estimators\": [50, 100, 200, 400], \"max_depth\": range(2, 6)},\n",
    "        \"random_state\": [random_state],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ExtraTreesRegressor\",\n",
    "        \"model\": ExtraTreesRegressor(),\n",
    "        \"params\": {\"n_estimators\": [50, 100, 200, 400], \"max_depth\": range(2, 6)},\n",
    "        \"random_state\": [random_state],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GradientBoostingRegressor\",\n",
    "        \"model\": GradientBoostingRegressor(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200, 400],\n",
    "            \"max_depth\": range(2, 6),\n",
    "            \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "            \"random_state\": [random_state],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"AdaBoostRegressor\",\n",
    "        \"model\": AdaBoostRegressor(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200, 400],\n",
    "            \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "            \"random_state\": [random_state],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SVR\",\n",
    "        \"model\": SVR(),\n",
    "        \"params\": {\n",
    "            \"kernel\": [\"linear\", \"rbf\"],\n",
    "            \"C\": [0.1, 1.0, 10.0],\n",
    "            \"epsilon\": [0.01, 0.1, 1.0],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"LinearRegression\",\n",
    "        \"model\": LinearRegression(),\n",
    "        \"params\": {},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Ridge\",\n",
    "        \"model\": Ridge(),\n",
    "        \"params\": {\"alpha\": [0.001, 0.01, 0.1, 1.0]},\n",
    "        \"random_state\": [random_state],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Lasso\",\n",
    "        \"model\": Lasso(),\n",
    "        \"params\": {\"alpha\": [0.001, 0.01, 0.1, 1.0]},\n",
    "        \"random_state\": [random_state],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ElasticNet\",\n",
    "        \"model\": ElasticNet(),\n",
    "        \"params\": {\"alpha\": [0.001, 0.01, 0.1, 1.0], \"l1_ratio\": [0.1, 0.5, 0.9]},\n",
    "        \"random_state\": [random_state],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"BayesianRidge\",\n",
    "        \"model\": BayesianRidge(),\n",
    "        \"params\": {\n",
    "            \"alpha_1\": [0.001, 0.01, 0.1, 1.0],\n",
    "            \"alpha_2\": [0.001, 0.01, 0.1, 1.0],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CatBoostRegressor\",\n",
    "        \"model\": CatBoostRegressor(verbose=False),\n",
    "        \"params\": {\n",
    "            \"iterations\": [50, 100, 200, 400],\n",
    "            \"depth\": range(2, 6),\n",
    "            \"learning_rate\": [0.001, 0.01, 0.1, 1.0],\n",
    "            \"random_state\": [random_state],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"XGBRegressor\",\n",
    "        \"model\": XGBRegressor(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200, 400],\n",
    "            \"max_depth\": range(2, 6),\n",
    "            \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "            \"random_state\": [random_state],\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = Path(\"/home/moritz/Workspace/masterthesis/model/custom_models/univariate/small\")\n",
    "\n",
    "eval_metric = \"mean_absolute_error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_drop = [\"MLPRegressor\", \"KNeighborsRegressor\", \"DecisionTreeRegressor\", \"SVR\", \"Lasso\", \"GradientBoostingRegressor\"]\n",
    "#models_to_drop = [\"MLPRegressor\", \"KNeighborsRegressor\"]\n",
    "\n",
    "for name in models_to_drop:\n",
    "    for i in range(len(MODELS)):\n",
    "        if MODELS[i][\"name\"] == name:\n",
    "            del MODELS[i]\n",
    "            break\n",
    "        \n",
    "for model in MODELS:\n",
    "    print(model[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "\n",
    "for perma_dim, X_final in feature_sets.items():\n",
    "    print(f\"PERMA dimension: {perma_dim}\")\n",
    "    # Run the hyperparameter search\n",
    "    models_path = SAVE_DIR / perma_dim\n",
    "    X_train = X_final[\"train\"]\n",
    "    search = HyperparaSearch(models=MODELS, metrics=[\"mean_absolute_error\"], models_path = models_path, n_folds=5, n_jobs=-1, mode=\"uni\")  \n",
    "    results = search.run(X_train, Y_train[perma_dim], save=True)\n",
    "    results_dict[perma_dim] = results\n",
    "    # Print the model with the lowest score\n",
    "    best_model = min(\n",
    "        [\n",
    "            min(\n",
    "                sublist,\n",
    "                key=lambda x: x[\"score\"]\n",
    "                if x[\"metric\"] == eval_metric\n",
    "                else float(\"inf\"),\n",
    "            )\n",
    "            for sublist in results\n",
    "        ],\n",
    "        key=lambda x: x[\"score\"],\n",
    "    )\n",
    "    print(perma_dim)\n",
    "    print(f\"Best model: {best_model['name']}\")\n",
    "    print(f\"Best params: {best_model['params']}\")\n",
    "    print(f\"Best Score: {best_model['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = {}\n",
    "\n",
    "for perma_dim, results in results_dict.items():\n",
    "    best_model = min(\n",
    "        [\n",
    "            min(\n",
    "                sublist,\n",
    "                key=lambda x: x[\"score\"]\n",
    "                if x[\"metric\"] == eval_metric\n",
    "                else float(\"inf\"),\n",
    "            )\n",
    "            for sublist in results\n",
    "        ],\n",
    "        key=lambda x: x[\"score\"],\n",
    "    )\n",
    "    if perma_dim == \"E\":\n",
    "        best_models[perma_dim] = \"AdaBoostRegressor\"\n",
    "    # print(perma_dim)\n",
    "    # print(f\"Best model: {best_model['name']}\")\n",
    "    # print(f\"Best params: {best_model['params']}\")\n",
    "    # print(f\"Best Score: {best_model['score']}\")\n",
    "    else:\n",
    "       best_models[perma_dim] = best_model['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_models = {}\n",
    "\n",
    "# for perma_dim, results in results_dict.items():\n",
    "#     flattened_results = [item for sublist in results for item in sublist]\n",
    "#     best_model = min(flattened_results, key=lambda x: x['score'])\n",
    "#     #if perma_dim == \"P\":\n",
    "#     #    best_models[perma_dim] = \"DecisionTreeClassifier\"\n",
    "#     # print(perma_dim)\n",
    "#     # print(f\"Best model: {best_model['name']}\")\n",
    "#     # print(f\"Best params: {best_model['params']}\")\n",
    "#     # print(f\"Best Score: {best_model['score']}\")\n",
    "#     #else:\n",
    "#     best_models[perma_dim] = best_model['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model(file_path: Path):\n",
    "\n",
    "    models = {}\n",
    "\n",
    "    if file_path.is_file():\n",
    "        model = joblib.load(file_path)\n",
    "\n",
    "        models[str(file_path.stem)] = model\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perma_models = {}\n",
    "\n",
    "for perma_dim in [\"P\", \"E\", \"R\", \"M\", \"A\"]:\n",
    "    models_path = SAVE_DIR / perma_dim / (best_models[perma_dim] + \".joblib\")\n",
    "    models = load_best_model(models_path)\n",
    "    perma_models[perma_dim] = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perma_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perma_models = {}\n",
    "\n",
    "# for perma_dim in [\"P\", \"E\", \"R\", \"M\", \"A\"]:\n",
    "#     models_path = SAVE_DIR / perma_dim\n",
    "#     models = load_models(models_path)\n",
    "#     perma_models[perma_dim] = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_models(models, X, y):\n",
    "    # Generate and return a dictionary of mean absolute error (MAE) scores and prediction arrays for each model\n",
    "    results = {}\n",
    "    for model_name, mae_grid_search in models.items():\n",
    "        # Fit the model\n",
    "        model = mae_grid_search[0].best_estimator_\n",
    "        if model_name == \"CatBoostRegressor\":\n",
    "            print(model.get_params())\n",
    "        else:\n",
    "            print(model)\n",
    "        \n",
    "for dim, models in perma_models.items():\n",
    "    X_test = feature_sets[dim][\"test\"]\n",
    "    print_models(models, X_test, Y_test[dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(models, X, y):\n",
    "    # Generate and return a dictionary of mean absolute error (MAE) scores and prediction arrays for each model\n",
    "    results = {}\n",
    "    for model_name, mae_grid_search in models.items():\n",
    "        # Fit the model\n",
    "        model = mae_grid_search[0].best_estimator_\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X)\n",
    "        # Calculate mean squared error and mean absolute error\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        results[model_name] = {\"mae\": mae, \"mse\": mse, \"y_pred\": y_pred}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_shap_values(models, X):\n",
    "#     for model_name, mae_grid_search in models.items():\n",
    "#         model = mae_grid_search[0].best_estimator_\n",
    "#         print(model_name)\n",
    "\n",
    "#         if model_name in [\"ElasticNet\", \"Lasso\", \"Ridge\", \"LinearRegression\", \"BayesianRidge\"]:\n",
    "#             explainer = shap.LinearExplainer(model, X, feature_names=X.columns)\n",
    "#             shap_values = explainer(X)\n",
    "#             shap.summary_plot(shap_values, X)\n",
    "#         elif model_name in [\"AdaBoostRegressor\"]:\n",
    "#             explainer = shap.SamplingExplainer(model.predict, X)\n",
    "#             shap_values = explainer(X)\n",
    "#             shap.summary_plot(shap_values, X)\n",
    "#         else:\n",
    "#             explainer = shap.Explainer(model, feature_names=X.columns)\n",
    "#             shap_values = explainer(X)\n",
    "#             shap.summary_plot(shap_values, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dim, models in perma_models.items():\n",
    "#     print(dim)\n",
    "#     X_test = feature_sets[dim][\"train\"]\n",
    "#     generate_shap_values(models, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shap_values_and_ranks(dim, models, X):\n",
    "    feature_rankings = {}\n",
    "\n",
    "    for i, (model_name, mae_grid_search) in enumerate(models.items()):\n",
    "        model = mae_grid_search[0].best_estimator_\n",
    "        print(model_name)\n",
    "        \n",
    "        if model_name in [\"ElasticNet\", \"Lasso\", \"Ridge\", \"LinearRegression\", \"BayesianRidge\"]:\n",
    "            explainer = shap.LinearExplainer(model, X, feature_names=X.columns)\n",
    "        elif model_name in [\"AdaBoostRegressor\", \"SVR\"]:\n",
    "            explainer = shap.SamplingExplainer(model.predict, X)\n",
    "        else:\n",
    "            explainer = shap.Explainer(model, feature_names=X.columns)\n",
    "\n",
    "        shap_values = explainer(X)\n",
    "        shap.summary_plot(shap_values, X, plot_size=(13, 3), plot_type='violin', show=False)\n",
    "        path = \"/home/moritz/Dropbox (Personal)/Dropbox/Apps/Overleaf/tum-thesis-latex/data/05_results/small_regr_shap\"\n",
    "        image = path + \"/\" + dim[i].lower() + \"_small_regr.pdf\"\n",
    "        plt.savefig(image, bbox_inches='tight', dpi=300)\n",
    "        plt.close()  # close the plot\n",
    "\n",
    "        # Get the mean absolute shap values for each feature\n",
    "        mean_abs_shap_values = np.abs(shap_values.values).mean(axis=0)\n",
    "\n",
    "        # Create a DataFrame for easy sorting and manipulation\n",
    "        feature_shap_df = pd.DataFrame(list(zip(X.columns, mean_abs_shap_values)), \n",
    "                                       columns=['feature','mean_abs_shap'])\n",
    "\n",
    "        # Sort by the mean absolute shap value and add rank column\n",
    "        feature_shap_df = feature_shap_df.sort_values('mean_abs_shap', ascending=False)\n",
    "        feature_shap_df['rank'] = range(1, len(feature_shap_df) + 1)\n",
    "        \n",
    "        # Store the ranking in the dictionary\n",
    "        feature_rankings[model_name] = feature_shap_df[['feature', 'rank']].set_index('feature').to_dict()['rank']\n",
    "\n",
    "    return feature_rankings\n",
    "\n",
    "def aggregate_ranks(feature_rankings_perma):\n",
    "    aggregated_ranks = {}\n",
    "    for dim, feature_rankings in feature_rankings_perma.items():\n",
    "        # Calculate the total rank and count for each feature\n",
    "        total_ranks = {}\n",
    "        counts = {}\n",
    "        for model_name, ranks in feature_rankings.items():\n",
    "            for feature, rank in ranks.items():\n",
    "                if feature in total_ranks:\n",
    "                    total_ranks[feature] += rank\n",
    "                    counts[feature] += 1\n",
    "                else:\n",
    "                    total_ranks[feature] = rank\n",
    "                    counts[feature] = 1\n",
    "\n",
    "        # Calculate the mean rank for each feature\n",
    "        mean_ranks = {feature: total_ranks[feature] / counts[feature] for feature in total_ranks}\n",
    "\n",
    "        # Sort the features by their mean ranks\n",
    "        sorted_features = sorted(mean_ranks, key=mean_ranks.get)\n",
    "        aggregated_ranks[dim] = sorted_features\n",
    "\n",
    "    return aggregated_ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_rankings_perma = {}\n",
    "for dim, models in perma_models.items():\n",
    "    print(dim)\n",
    "    X_test = feature_sets[dim][\"train\"]\n",
    "    X_test.columns = [col.lower() for col in X_test.columns]\n",
    "    X_test.rename(columns={'degree centrality': 'out_degree_centrality'}, inplace=True)\n",
    "    X_test.rename(columns=lambda x: x.split('_')[0].replace('gazedifference', 'gaze_difference') + '_' + x.split('_')[1] if 'gazedifference_' in x else x, inplace=True)\n",
    "\n",
    "    feature_rankings_perma[dim] = generate_shap_values_and_ranks(dim, models, X_test)\n",
    "\n",
    "aggregated_ranks = aggregate_ranks(feature_rankings_perma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_importance(feature_rankings_perma):\n",
    "    # Initialize total ranks and counts dictionaries\n",
    "    total_ranks = {}\n",
    "    counts = {}\n",
    "\n",
    "    # Iterate over all dimensions\n",
    "    for dim, feature_rankings in feature_rankings_perma.items():\n",
    "        # Iterate over all features in this dimension\n",
    "        for rank, feature in enumerate(feature_rankings, 1):\n",
    "            # Update total ranks and counts\n",
    "            if feature in total_ranks:\n",
    "                val = 1.1 - 0.1*rank\n",
    "                total_ranks[feature] += val\n",
    "                counts[feature] += 1\n",
    "            else:\n",
    "                val = 1.1 - 0.1*rank\n",
    "                total_ranks[feature] = val\n",
    "                counts[feature] = 1\n",
    "\n",
    "\n",
    "    # Calculate the average rank for each feature\n",
    "    #avg_ranks = {feature: total_ranks[feature] / counts[feature] for feature in total_ranks}\n",
    "\n",
    "    return counts, total_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply your function to the data\n",
    "counts, total_ranks = calculate_importance(aggregated_ranks)\n",
    "\n",
    "# Sort the dictionary by value\n",
    "sorted_importance = sorted(total_ranks.items(), key=lambda item: item[1])\n",
    "\n",
    "# Unpack names and values\n",
    "names, values = zip(*sorted_importance)\n",
    "\n",
    "# Create a color palette\n",
    "palette = sns.color_palette(\"muted\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Apply color palette\n",
    "ax.set_prop_cycle(color=palette)\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "ax.barh(names, values)\n",
    "\n",
    "# Add counts on top of the bars\n",
    "for i, name in enumerate(names):\n",
    "    count = counts[name]\n",
    "    value = values[i]\n",
    "    ax.text(value, i, f' #{count}', va='center', fontsize=14)  # increase fontsize here\n",
    "\n",
    "ax.set_xlabel('Importance', fontsize=14)  # increase fontsize here\n",
    "plt.yticks(fontsize=14) \n",
    "plt.xticks(fontsize=14) \n",
    "\n",
    "\n",
    "path = \"/home/moritz/Dropbox (Personal)/Dropbox/Apps/Overleaf/tum-thesis-latex/data/05_results\"\n",
    "image = path + \"/imp_regr.pdf\"\n",
    "plt.savefig(image, bbox_inches='tight', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perma_results = {}\n",
    "\n",
    "for dim, models in perma_models.items():\n",
    "    X_test = feature_sets[dim][\"test\"]\n",
    "    print(X_test.columns)\n",
    "    results = generate_predictions(models, X_test, Y_test[dim])\n",
    "    perma_results[dim] = results\n",
    "    # for model_name, result in results.items():\n",
    "    #     print(f\"{dim} - {model_name}: MAE - {result['mae']}, MSE - {result['mse']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = {}\n",
    "\n",
    "for dim, results in perma_results.items():\n",
    "    curr_mae = float(\"inf\")\n",
    "    for model in results:\n",
    "        if results[model][\"mae\"] < curr_mae:\n",
    "            curr_mae = results[model][\"mae\"]\n",
    "            best_results[dim] = {\"model\": model, \"mae\": results[model][\"mae\"], \"preds\": results[model][\"y_pred\"]}\n",
    "            \n",
    "print(best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_feats_dict = {}\n",
    "\n",
    "# for dim, results in results_dict.items():\n",
    "#     best_feats_dict[dim] = {model[0]['name']: {\"feat_imp\": model[0]['best_feats'], \"score\": model[0][\"score\"]}  for model in results}\n",
    "\n",
    "# best_feats_dict_dim = {}\n",
    "    \n",
    "# for dim in ['P', 'E', 'R', 'M', 'A']:\n",
    "#     model = best_results[dim][\"model\"]\n",
    "#     feat_imp = abs(best_feats_dict[dim][model][\"feat_imp\"])\n",
    "#     best_feats_dict_dim[dim] = feat_imp\n",
    "\n",
    "# best_feats_dict_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranked_feats_dict = {}\n",
    "\n",
    "# for (dim, feat_imp_vals), (dim, X_final) in zip(best_feats_dict_dim.items(), feature_sets.items()):\n",
    "#     # Map the feature importance values with the feature list using a dictionary comprehension\n",
    "#     column_names = X_final[\"train\"].columns.tolist()\n",
    "#     feat_imp_map = {column_names[i]: feat_imp_vals[i] for i in range(len(column_names))}\n",
    "#     # Rank the features by their importance value in descending order\n",
    "#     ranked_feats = sorted(feat_imp_map.items(), key=lambda x: x[1], reverse=True)\n",
    "#     ranked_feats_dict[dim] = ranked_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Separate the feature names and importance scores for each dimension PERMA\n",
    "# P_features, P_scores = zip(*ranked_feats_dict[\"P\"])\n",
    "# E_features, E_scores = zip(*ranked_feats_dict[\"E\"])\n",
    "# R_features, R_scores = zip(*ranked_feats_dict[\"R\"])\n",
    "# M_features, M_scores = zip(*ranked_feats_dict[\"M\"])\n",
    "# A_features, A_scores = zip(*ranked_feats_dict[\"A\"])\n",
    "\n",
    "\n",
    "# #Create a bar plot for each dimension PERMA\n",
    "# fig, axs = plt.subplots(1, 5, figsize=(18, 6))\n",
    "# axs[0].bar(P_features, P_scores)\n",
    "# axs[0].set_title(\"P\")\n",
    "# axs[1].bar(E_features, E_scores)\n",
    "# axs[1].set_title(\"E\")\n",
    "# axs[2].bar(R_features, R_scores)\n",
    "# axs[2].set_title(\"R\")\n",
    "# axs[3].bar(M_features, M_scores)\n",
    "# axs[3].set_title(\"M\")\n",
    "# axs[4].bar(A_features, A_scores)\n",
    "# axs[4].set_title(\"A\")\n",
    "\n",
    "# #Set common axis labels and title\n",
    "# fig.suptitle(\"Feature Importance for each PERMA Dimension\", fontsize=16)\n",
    "# for ax in axs:\n",
    "#     ax.set_xticklabels(ax.get_xticklabels(), rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline:\n",
    "Y_baseline = np.full_like(Y_test, Y_test.mean(axis=0).transpose())\n",
    "mae_baseline = mean_absolute_error(Y_test, Y_baseline, multioutput='raw_values')\n",
    "\n",
    "print(f\"Baseline MAE for each dimension: {mae_baseline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the errors between the actual values and the baseline predictions\n",
    "baseline_errors = abs(Y_test - Y_baseline)\n",
    "\n",
    "# Calculate the errors for each model and each dimension\n",
    "model_errors = pd.DataFrame()\n",
    "\n",
    "# Calculate the errors for each model and each dimension\n",
    "for dim in ['P', 'E', 'R', 'M', 'A']:\n",
    "    err = abs(Y_test[dim] - best_results[dim]['preds'])\n",
    "    errors = pd.DataFrame({'errors': err})\n",
    "    model_errors = pd.concat([model_errors, errors], axis=1)\n",
    "\n",
    "# Concatenate baseline and model errors\n",
    "all_errors = [baseline_errors, model_errors]\n",
    "\n",
    "# Create a combined boxplot\n",
    "fig, ax = plt.subplots()\n",
    "palette = sns.color_palette(\"muted\")\n",
    "ax.set_prop_cycle(color=palette)\n",
    "colors = ['lightblue', 'orange']\n",
    "labels = ['Baseline', 'Models']\n",
    "\n",
    "for i, error_df in enumerate(all_errors):\n",
    "    position = np.arange(1, len(error_df.columns) + 1) + (i * 0.4)\n",
    "    #ax.boxplot(error_df, positions=position, widths=0.4, patch_artist=True, boxprops=dict(facecolor=colors[i]), medianprops=dict(color='black'))\n",
    "    ax.violinplot(error_df, positions=position, widths=0.4, showmedians=False, showextrema=False, showmeans=True)\n",
    "\n",
    "# Define the labels for the x-axis\n",
    "perma = ['P', 'E', 'R', 'M', 'A']\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "ax.set_xticks(np.arange(1, len(perma) + 1))\n",
    "ax.set_xticklabels(perma)\n",
    "ax.set_ylim(-0.02, 0.63)\n",
    "\n",
    "# Create the legend patches and labels\n",
    "patches = [mpatches.Patch(facecolor=color, label=label) for color, label in zip(colors, labels)]\n",
    "\n",
    "# Add the legend to the plot\n",
    "#ax.legend()\n",
    "ax.set_title('MAE-scores on PERMA dimensions')\n",
    "ax.legend(handles=patches, loc='upper right')\n",
    "ax.set_xlabel('PERMA Dimensions')\n",
    "ax.set_ylabel('MAE scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the best model for each PERMA dimension\n",
    "best_mae_values = [v['mae'] for v in best_results.values()]\n",
    "#best_mae_values = [0.162, 0.205, 0.131, 0.224, 0.133]\n",
    "\n",
    "# regression big simulation\n",
    "#best_mae_values = [0.143, 0.241, 0.168, 0.212, 0.138]\n",
    "#models = [\"BayesianRidge\", \"BayesianRidge\", \"BayesianRidge\", \"BayesianRidge\", \"BayesianRidge\"]\n",
    "\n",
    "# Define the x-axis labels and the bar width\n",
    "perma_dimensions = ['P', 'E', 'R', 'M', 'A']\n",
    "bar_width = 0.35\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots()\n",
    "palette = sns.color_palette(\"muted\")\n",
    "ax.set_prop_cycle(color=palette)\n",
    "ax.bar(np.arange(len(perma_dimensions)), mae_baseline, width=bar_width, label='Baseline')\n",
    "ax.bar(np.arange(len(perma_dimensions))+bar_width, best_mae_values, width=bar_width, label='Best Models')\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "ax.set_xticks(np.arange(len(perma_dimensions))+bar_width/2)\n",
    "ax.set_xticklabels(perma_dimensions)\n",
    "ax.set_xlabel('PERMA Dimensions', fontsize=20)\n",
    "\n",
    "# Set the y-axis label and limits\n",
    "ax.set_ylabel('MAE scores', fontsize=20)\n",
    "ax.set_ylim([0, max(np.max(mae_baseline), np.max(best_mae_values))*1.1])\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "# Add the model names and MAE values above each bar\n",
    "for i, v in enumerate(mae_baseline):\n",
    "    ax.text(i, 0.005, f\"MAE: {v:.3f}\", rotation=90, ha='center', va='bottom', fontsize=15, color='white')\n",
    "    model = best_results[perma_dimensions[i]]['model']\n",
    "    #model = models[i]\n",
    "    ax.text(i+bar_width, 0.005, f\"MAE: {best_mae_values[i]:.3f} ({model})\", rotation=90, ha='center', va='bottom', fontsize=15, color='black')\n",
    "\n",
    "# Add a legend and title\n",
    "ax.legend(fontsize=15)\n",
    "ax.set_title('MAE-scores on the small data set', fontsize=20)\n",
    "ax.set_ylim([0, 0.3])\n",
    "\n",
    "path = \"/home/moritz/Dropbox (Personal)/Dropbox/Apps/Overleaf/tum-thesis-latex/data/05_results\"\n",
    "image = path + \"/mae_small.pdf\"\n",
    "plt.savefig(image, bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pd.DataFrame()\n",
    "\n",
    "for dim in ['P', 'E', 'R', 'M', 'A']:\n",
    "    err = best_results[dim]['preds']\n",
    "    errors = pd.DataFrame({dim: err})\n",
    "    Y_pred = pd.concat([Y_pred, errors], axis=1)\n",
    "\n",
    "# Unscale the data back to the original scale\n",
    "Y_preds = pd.DataFrame(minmax_scaler.inverse_transform(Y_pred), columns=['P', 'E', 'R', 'M', 'A'])\n",
    "Y_tests = pd.DataFrame(minmax_scaler.inverse_transform(Y_test), columns=['P', 'E', 'R', 'M', 'A'])\n",
    "Y_baselines = pd.DataFrame(minmax_scaler.inverse_transform(Y_baseline), columns=['P', 'E', 'R', 'M', 'A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_baseline_df = pd.DataFrame(Y_baseline, columns=['P', 'E', 'R', 'M', 'A'])\n",
    "\n",
    "print(Y_pred.min())\n",
    "print(Y_pred.max())\n",
    "print(Y_test.min())\n",
    "print(Y_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins and labels\n",
    "bins = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "labels = ['0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']\n",
    "\n",
    "# bins = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "# labels = ['0-0.1', '0.1-0.2', '0.2-0.3', '0.3-0.4', '0.4-0.5', '0.5-0.6', '0.6-0.7', '0.7-0.8', '0.8-0.9', '0.9-1.0']\n",
    "\n",
    "# Apply the binning\n",
    "actual_values_bins = Y_test.apply(pd.cut, bins=bins, labels=labels)\n",
    "predictions_bins = Y_pred.apply(pd.cut, bins=bins, labels=labels)\n",
    "baseline_bins = Y_baseline_df.apply(pd.cut, bins=bins, labels=labels)\n",
    "\n",
    "# Reset the indices of both DataFrames\n",
    "actual_values_bins.reset_index(drop=True, inplace=True)\n",
    "predictions_bins.reset_index(drop=True, inplace=True)\n",
    "baseline_bins.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Calculate the accuracy for each column\n",
    "accur_preds = (actual_values_bins == predictions_bins).mean()\n",
    "accur_baseline = (actual_values_bins == baseline_bins).mean()\n",
    "\n",
    "# Combine accuracy values into a single DataFrame\n",
    "accuracy_df = pd.DataFrame({'Predictions': accur_preds, 'Baseline': accur_baseline})\n",
    "\n",
    "# Plot the accuracy values for predictions and baseline\n",
    "ax = accuracy_df.plot(kind='bar', figsize=(10, 5))\n",
    "plt.title('Accuracy for Predictions and Baseline')\n",
    "plt.xlabel('PERMA Dimensions')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylim(0, 0.7)\n",
    "plt.legend()\n",
    "\n",
    "# Annotate values on top of the bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type='edge', fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels for the radar plots\n",
    "labels = ['P', 'E', 'R', 'M', 'A']\n",
    "\n",
    "# Create the figure and subplot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111, polar=True)\n",
    "\n",
    "# Loop over each row of Y_tests and Y_preds\n",
    "for i in [np.random.randint(0, len(Y_tests))]:\n",
    "    # Define the data for the radar plot\n",
    "    data1 = Y_tests.iloc[i].values\n",
    "    data2 = Y_preds.iloc[i].values\n",
    "    #data3 = Y_baselines.iloc[i].values\n",
    "\n",
    "    # Define the angle for each label\n",
    "    angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False)\n",
    "\n",
    "    # Close the plot\n",
    "    data1 = np.concatenate((data1, [data1[0]]))\n",
    "    angles = np.concatenate((angles, [angles[0]]))\n",
    "    data2 = np.concatenate((data2, [data2[0]]))\n",
    "    #data3 = np.concatenate((data3, [data3[0]]))\n",
    "\n",
    "    # Plot the radar plot\n",
    "    ax.plot(angles, data1, 'o-', linewidth=2, label=f'Target')\n",
    "    ax.fill(angles, data1, alpha=0.25)\n",
    "    ax.plot(angles, data2, 'o-', linewidth=2, label=f'Prediction')\n",
    "    ax.fill(angles, data2, alpha=0.25)\n",
    "    #ax.plot(angles, data3, 'o-', linewidth=2, label=f'Baseline')\n",
    "    #ax.fill(angles, data3, alpha=0.25)\n",
    "\n",
    "# Set the labels\n",
    "ax.set_thetagrids(angles[:-1] * 180/np.pi, labels)\n",
    "plt.yticks(np.arange(0, 7.1, 1.0))\n",
    "plt.ylim(0, 7)\n",
    "plt.title('Comparison of PERMA Scores', fontsize=14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emorec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c67f0142598d7b06f34a2acb15d98e76cb3b1e66b7b86f21a91e42da5de8188"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

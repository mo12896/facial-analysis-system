{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, chi2, mutual_info_regression, SelectPercentile, VarianceThreshold\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import multiprocessing\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "\n",
    "\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from src.emotion.prediction.aggregates.train import HyperparaSearch\n",
    "from src.emotion.prediction.aggregates.models import MODELS\n",
    "from src.emotion.prediction.aggregates.test import load_models, generate_predictions, plot_predictions\n",
    "from src.emotion.utils.constants import DATA_DIR\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('/home/moritz/Workspace/masterthesis/data/features_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_csv('/home/moritz/Workspace/masterthesis/data/perma_scores_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 9351)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(features, targets, on=[\"E-Mail-Adresse\", \"Day\"])\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104, 9348)\n"
     ]
    }
   ],
   "source": [
    "# Handle Missing Values\n",
    "\n",
    "df.dropna(axis=1, how='any', inplace=True)\n",
    "#df = dataset.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers\n",
    "\n",
    "# Check if all PERMA values are the same in each row\n",
    "same_PERMA = (df['P'] == df['E']) & (df['E'] == df['R']) & (df['R'] == df['M']) & (df['M'] == df['A'])\n",
    "# Remove the rows where all PERMA values are the same\n",
    "df = df[~same_PERMA]\n",
    "print(df.shape)\n",
    "\n",
    "# find columns where all values are the same\n",
    "cols_to_drop = [col for col in df.columns if df[col].nunique() == 1]\n",
    "# drop the columns\n",
    "df = df.drop(cols_to_drop, axis=1)\n",
    "print(df.shape)\n",
    "\n",
    "# drop columns where all values are only 0 or 1\n",
    "df = df.loc[:, ~(df.isin([0, 1]).all() & ~df.isin([0, 1]).any())]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load X and Y\n",
    "# Store the PERMA values in Y\n",
    "Y = df[['P', 'E', 'R', 'M', 'A']]\n",
    "\n",
    "# Store the other columns in X\n",
    "X = df.drop(columns=['ClassID', 'E-Mail-Adresse', 'Day', 'First Name', 'Last Name/Surname', 'P', 'E', 'R', 'M', 'A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Features\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the dataframe and transform the dataframe\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(matrix):\n",
    "    # center the matrix\n",
    "    matrix = matrix - np.mean(matrix, axis=0)\n",
    "\n",
    "    # transpose the matrix\n",
    "    matrix_t = matrix.T\n",
    "\n",
    "    # compute the correlation matrix using np.corrcoef\n",
    "    corr_matrix = np.corrcoef(matrix_t)\n",
    "\n",
    "    # create a heatmap of the correlation matrix using seaborn\n",
    "    sns.set(font_scale=0.7)\n",
    "    sns.heatmap(corr_matrix, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_correlation_matrix(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection: runs in ~ 5min \n",
    "# Step 1: Identify feature clusters\n",
    "# Create a dendrogram using hierarchical clustering\n",
    "linkage = hierarchy.linkage(corr_matrix, method='complete')\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Data points')\n",
    "plt.ylabel('Distance')\n",
    "hierarchy.dendrogram(\n",
    "    linkage,\n",
    "    leaf_rotation=0.,  # Rotate x-axis labels\n",
    "    leaf_font_size=12.,  # Font size for x-axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clusters from the dendrogram\n",
    "max_d = 15 # Maximum distance between clusters\n",
    "clusters = hierarchy.fcluster(linkage, max_d, criterion='distance')\n",
    "\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group columns by cluster\n",
    "df = pd.DataFrame(corr_matrix)\n",
    "df.columns = ['col_' + str(i) for i in range(df.shape[1])]\n",
    "df['cluster'] = clusters\n",
    "grouped = df.groupby('cluster')\n",
    "\n",
    "# Get the size of each group\n",
    "group_sizes = grouped.size()\n",
    "\n",
    "# Plot the group sizes\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(x=group_sizes.index, height=group_sizes.values, color='blue')\n",
    "plt.title('Group Sizes')\n",
    "plt.xlabel('Group')\n",
    "plt.ylabel('Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y.shape)\n",
    "print(X.shape)\n",
    "print(len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate the correlation matrix between the columns of X and the columns of Y\n",
    "corr_matrix = np.abs(np.corrcoef(X.T, Y.T)[:X.shape[1], X.shape[1]:])\n",
    "\n",
    "# compute the row-wise averages of the matrix\n",
    "avg_matrix = np.mean(corr_matrix, axis=1, keepdims=True)\n",
    "\n",
    "# concatenate the average matrix with the group array\n",
    "concat_matrix = np.concatenate([avg_matrix, clusters.reshape(len(clusters), 1)], axis=1)\n",
    "\n",
    "# sort the concatenated matrix by group\n",
    "sorted_matrix = concat_matrix[concat_matrix[:, -1].argsort()]\n",
    "\n",
    "# find the maximum value in each group and its index\n",
    "max_values = []\n",
    "# iterate over the unique groups in the second column of the sorted matrix\n",
    "for group in np.unique(sorted_matrix[:, 1]):\n",
    "    # find the indices of rows that belong to the current group\n",
    "    indices = np.where(sorted_matrix[:, 1] == group)[0]\n",
    "    # get the maximum value in the first column for the current group\n",
    "    max_value = np.max(sorted_matrix[indices, 0])\n",
    "    # append the maximum value to the list\n",
    "    max_values.append(max_value)\n",
    "    \n",
    "    \n",
    "# find the indices of all the maximum values in the avg_matrix\n",
    "max_indices = []\n",
    "for max_value in max_values:\n",
    "    indices = np.where(avg_matrix == max_value)[0]\n",
    "    max_indices.extend(indices)\n",
    "\n",
    "X_filtered = X.iloc[:, max_indices]\n",
    "print(X_filtered.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_matrix(X_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ensemble feature selection (using voting) across multiple filter methods\n",
    "# # TODO: Maybe create seperate feature sets for each PERMA dimension seperatly?\n",
    "# def get_selected_features(Y, X_filtered, filter_methods):\n",
    "#     selected_features = []\n",
    "    \n",
    "#     for i in range(Y.shape[1]):\n",
    "#         y_i = Y.iloc[:, i]\n",
    "#         selected_i = []\n",
    "        \n",
    "#         for method_name, method in filter_methods.items():\n",
    "#             pipeline = Pipeline([(method_name, method), ('regressor', LinearRegression())])\n",
    "#             pipeline.fit(X_filtered, y_i)\n",
    "#             selected_i.append(pipeline.named_steps[method_name].get_support(indices=True))\n",
    "        \n",
    "#         selected_i = np.concatenate(selected_i)\n",
    "#         selected_i = np.unique(selected_i)\n",
    "#         selected_features.append(selected_i)\n",
    "        \n",
    "#     return np.concatenate(selected_features)\n",
    "\n",
    "# filter_methods = {\n",
    "#     'f_regression': SelectKBest(f_regression, k=1),\n",
    "#     'mutual_info_regression': SelectKBest(mutual_info_regression, k=1),\n",
    "#     'variance_threshold': VarianceThreshold(threshold=0.1),\n",
    "# }\n",
    "\n",
    "# all_selected_features = get_selected_features(Y, X_filtered, filter_methods)\n",
    "\n",
    "# print(\"Fused selected features:\", all_selected_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_voting_features_multi(Y, X_filtered, filter_methods, k):\n",
    "    feature_counts = np.zeros(X_filtered.shape[1])\n",
    "    \n",
    "    #print(Y.shape[1])\n",
    "    print(len(Y.T))\n",
    "    \n",
    "    for i in range(len(Y.T)):\n",
    "        y_i = Y.iloc[:, i]\n",
    "        \n",
    "        for method_name, method in filter_methods.items():\n",
    "            pipeline = Pipeline([(method_name, method), ('regressor', LinearRegression())])\n",
    "            pipeline.fit(X_filtered, y_i)\n",
    "            selected_i = pipeline.named_steps[method_name].get_support(indices=True)\n",
    "            \n",
    "            # Increment the count for each selected feature\n",
    "            for index in selected_i:\n",
    "                feature_counts[index] += 1\n",
    "                \n",
    "    # Get the indices of the top k features with the most counts\n",
    "    top_k_features = np.argsort(feature_counts)[-k:]\n",
    "    \n",
    "    return top_k_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_methods = {\n",
    "    'f_regression': SelectKBest(f_regression, k=10),\n",
    "    'mutual_info_regression': SelectKBest(mutual_info_regression, k=10),\n",
    "    'variance_threshold': VarianceThreshold(threshold=0.1),\n",
    "}\n",
    "\n",
    "all_selected_features = get_selected_voting_features_multi(Y, X_filtered, filter_methods, k=15)\n",
    "\n",
    "print(\"Fused selected features:\", all_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = X_filtered.iloc[:, list(set(all_selected_features))]\n",
    "plot_correlation_matrix(X_final)\n",
    "column_names = X_final.columns.tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_drop = [\"MLPRegressor\", \"KNeighborsRegressor\", \"DecisionTreeRegressor\", \"GradientBoostingRegressor\", \"SVR\"]\n",
    "\n",
    "for name in models_to_drop:\n",
    "    for i in range(len(MODELS)):\n",
    "        if MODELS[i][\"name\"] == name:\n",
    "            del MODELS[i]\n",
    "            break\n",
    "        \n",
    "for model in MODELS:\n",
    "    print(model[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs in ~5 min for n_fols=5\n",
    "search = HyperparaSearch(models=MODELS, metrics=[\"mean_absolute_error\"], n_folds=5, n_jobs=-1)\n",
    "\n",
    "results = search.run(X_final, Y, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feats_dict = {model[0]['name']: model[0]['best_feats'] for model in results}\n",
    "#print(best_feats_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in best_feats_dict.keys():\n",
    "    print(f\"Algorithm: {model}\")\n",
    "    # Get the feature importance values for the algorithm\n",
    "    feat_imp_vals = best_feats_dict[model]\n",
    "    # Map the feature importance values with the feature list using a dictionary comprehension\n",
    "    feat_imp_map = {column_names[i]: feat_imp_vals[i] for i in range(len(column_names))}\n",
    "    # Rank the features by their importance value in descending order\n",
    "    ranked_feats = sorted(feat_imp_map.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Print the ranked features\n",
    "    #print(ranked_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = \"mean_absolute_error\"\n",
    "\n",
    "# Plot the results\n",
    "mae_scores = [\n",
    "    rd[\"score\"]\n",
    "    for result_list in results\n",
    "    for rd in result_list\n",
    "    if rd[\"metric\"] == eval_metric\n",
    "]\n",
    "model_names = [\n",
    "    rd[\"name\"]\n",
    "    for result_list in results\n",
    "    for rd in result_list\n",
    "    if rd[\"metric\"] == eval_metric\n",
    "]\n",
    "plt.bar(model_names, mae_scores)\n",
    "plt.title(\"Mean Absolute Error Scores\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model with the lowest score\n",
    "best_model = min(\n",
    "    [\n",
    "        min(\n",
    "            sublist,\n",
    "            key=lambda x: x[\"score\"]\n",
    "            if x[\"metric\"] == eval_metric\n",
    "            else float(\"inf\"),\n",
    "        )\n",
    "        for sublist in results\n",
    "    ],\n",
    "    key=lambda x: x[\"score\"],\n",
    ")\n",
    "print(f\"Best model: {best_model['name']}\")\n",
    "print(f\"Best params: {best_model['params']}\")\n",
    "print(f\"Best Score: {best_model['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_voting_features_uni(Y, X_filtered, filter_methods, k):\n",
    "    feature_counts = np.zeros(X_filtered.shape[1])\n",
    "        \n",
    "    for method_name, method in filter_methods.items():\n",
    "        pipeline = Pipeline([(method_name, method), ('regressor', LinearRegression())])\n",
    "        pipeline.fit(X_filtered, Y)\n",
    "        selected_i = pipeline.named_steps[method_name].get_support(indices=True)\n",
    "        \n",
    "        # Increment the count for each selected feature\n",
    "        for index in selected_i:\n",
    "            feature_counts[index] += 1\n",
    "                \n",
    "    # Get the indices of the top k features with the most counts\n",
    "    top_k_features = np.argsort(feature_counts)[-k:]\n",
    "    \n",
    "    return top_k_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for each PERMA pillar\n",
    "perma_dict = {\n",
    "    \"P\": None,\n",
    "    \"E\": None,\n",
    "    \"R\": None,\n",
    "    \"M\": None,\n",
    "    \"A\": None,\n",
    "}\n",
    "\n",
    "filter_methods = {\n",
    "    'f_regression': SelectKBest(f_regression, k=10),\n",
    "    'mutual_info_regression': SelectKBest(mutual_info_regression, k=10),\n",
    "    'variance_threshold': VarianceThreshold(threshold=0.1),\n",
    "}\n",
    "\n",
    "# Iterate over each PERMA dimension in Y\n",
    "for i, perma_dim in enumerate(Y.columns):\n",
    "    # Get the selected features for the current PERMA dimension\n",
    "    selected_features = get_selected_voting_features_uni(Y.iloc[:, i], X_filtered, filter_methods, k=10)\n",
    "    # Save the selected features in the corresponding dictionary for the current PERMA pillar\n",
    "    perma_dict[perma_dim] = selected_features\n",
    "\n",
    "# Print the selected features for each PERMA dimension in each PERMA pillar\n",
    "for pillar in perma_dict:\n",
    "    print(pillar, \":\", perma_dict[pillar])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = {}\n",
    "\n",
    "for perma_dim, selected_features in perma_dict.items():\n",
    "    # Select the corresponding columns of X_filtered\n",
    "    X_final = X_filtered.iloc[:, list(set(selected_features))]\n",
    "    # Add the selected features for the current PERMA dimension to the feature sets dictionary\n",
    "    feature_sets[perma_dim] = X_final\n",
    "    # Print the names of the selected features\n",
    "    column_names = X_final.columns.tolist()\n",
    "    print(perma_dim, \":\", column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_final, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = Path(\"/home/moritz/Workspace/masterthesis/model/custom_models/univariate/big\")\n",
    "\n",
    "eval_metric = \"mean_absolute_error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_drop = [\"MLPRegressor\", \"KNeighborsRegressor\", \"DecisionTreeRegressor\", \"SVR\"]\n",
    "\n",
    "for name in models_to_drop:\n",
    "    for i in range(len(MODELS)):\n",
    "        if MODELS[i][\"name\"] == name:\n",
    "            del MODELS[i]\n",
    "            break\n",
    "        \n",
    "for model in MODELS:\n",
    "    print(model[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for perma_dim, X_final in feature_sets.items():\n",
    "    print(f\"PERMA dimension: {perma_dim}\")\n",
    "    # Run the hyperparameter search\n",
    "    models_path = SAVE_DIR / perma_dim\n",
    "    search = HyperparaSearch(models=MODELS, metrics=[\"mean_absolute_error\"], models_path = models_path, n_folds=5, n_jobs=-1, mode=\"uni\")\n",
    "    results = search.run(X_train, Y_train[perma_dim], save=True)\n",
    "    # Print the model with the lowest score\n",
    "    best_model = min(\n",
    "        [\n",
    "            min(\n",
    "                sublist,\n",
    "                key=lambda x: x[\"score\"]\n",
    "                if x[\"metric\"] == eval_metric\n",
    "                else float(\"inf\"),\n",
    "            )\n",
    "            for sublist in results\n",
    "        ],\n",
    "        key=lambda x: x[\"score\"],\n",
    "    )\n",
    "    print(perma_dim)\n",
    "    print(f\"Best model: {best_model['name']}\")\n",
    "    print(f\"Best params: {best_model['params']}\")\n",
    "    print(f\"Best Score: {best_model['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perma_models = {}\n",
    "\n",
    "for perma_dim in [\"P\", \"E\", \"R\", \"M\", \"A\"]:\n",
    "    print(perma_dim)\n",
    "    models_path = SAVE_DIR / perma_dim\n",
    "    models = load_models(models_path)\n",
    "    perma_models[perma_dim] = models\n",
    "    print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def generate_predictions(models, X, y):\n",
    "    # Generate and return a dictionary of mean absolute error (MAE) scores and prediction arrays for each model\n",
    "    results = {}\n",
    "    for model_name, mae_grid_search in models.items():\n",
    "        # Fit the model\n",
    "        model = mae_grid_search[0].best_estimator_\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X)\n",
    "        # Calculate mean squared error and mean absolute error\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        results[model_name] = {\"mae\": mae, \"mse\": mse, \"y_pred\": y_pred}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perma_results = {}\n",
    "\n",
    "for dim, models in perma_models.items():\n",
    "    results = generate_predictions(models, X_test, Y_test[dim])\n",
    "    perma_results[dim] = results\n",
    "    for model_name, result in results.items():\n",
    "        print(f\"{dim} - {model_name}: MAE - {result['mae']}, MSE - {result['mse']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = {}\n",
    "\n",
    "for dim, results in perma_results.items():\n",
    "    curr_mae = float(\"inf\")\n",
    "    for model in results:\n",
    "        if results[model][\"mae\"] < curr_mae:\n",
    "            curr_mae = results[model][\"mae\"]\n",
    "            best_results[dim] = {\"model\": model, \"mae\": results[model][\"mae\"]}\n",
    "            \n",
    "print(best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline:\n",
    "Y_baseline = np.full_like(Y, Y.mean())\n",
    "mae_baseline = mean_absolute_error(Y, Y_baseline, multioutput='raw_values')\n",
    "\n",
    "print(f\"Baseline MAE for each dimension: {mae_baseline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best model for each PERMA dimension\n",
    "best_mae_values = [v['mae'] for v in best_results.values()]\n",
    "\n",
    "# Define the x-axis labels and the bar width\n",
    "perma_dimensions = ['P', 'E', 'R', 'M', 'A']\n",
    "bar_width = 0.35\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(np.arange(len(perma_dimensions)), mae_baseline, width=bar_width, label='Baseline')\n",
    "ax.bar(np.arange(len(perma_dimensions))+bar_width, best_mae_values, width=bar_width, label='Best Models')\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "ax.set_xticks(np.arange(len(perma_dimensions))+bar_width/2)\n",
    "ax.set_xticklabels(perma_dimensions)\n",
    "ax.set_xlabel('PERMA Dimension')\n",
    "\n",
    "# Set the y-axis label and limits\n",
    "ax.set_ylabel('MAE')\n",
    "ax.set_ylim([0, max(np.max(mae_baseline), np.max(best_mae_values))*1.1])\n",
    "\n",
    "# Add the model names and MAE values above each bar\n",
    "for i, v in enumerate(mae_baseline):\n",
    "    ax.text(i, v+0.01, f\"MAE: {v:.2f}\", rotation=90, ha='center', va='bottom', fontsize=8)\n",
    "    ax.text(i+bar_width, best_mae_values[i]+0.01, f\"{best_results[perma_dimensions[i]]['model']}\\nMAE: {best_mae_values[i]:.2f}\", rotation=90, ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Add a legend and title\n",
    "ax.legend()\n",
    "ax.set_title('PERMA Dimension MAE Scores')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"P\": \"LinearRegression\",\n",
    "    \"E\": \"LinearRegression\",\n",
    "    \"R\": \"Lasso\",\n",
    "    \"M\": \"LinearRegression\",\n",
    "    \"A\": \"LinearRegression\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['P', 'E', 'R', 'M', 'A']\n",
    "data = np.array([0.5, 0.8, 0.6, 0.4, 0.9])\n",
    "\n",
    "# Calculate the angle for each label\n",
    "angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False)\n",
    "\n",
    "# Close the plot\n",
    "data = np.concatenate((data, [data[0]]))\n",
    "angles = np.concatenate((angles, [angles[0]]))\n",
    "\n",
    "# Create the figure\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111, polar=True)\n",
    "\n",
    "# Plot the data\n",
    "ax.plot(angles, data, 'o-', linewidth=2)\n",
    "ax.fill(angles, data, alpha=0.25)\n",
    "\n",
    "# Set the labels\n",
    "ax.set_thetagrids(angles[:-1] * 180/np.pi, labels)\n",
    "plt.yticks(np.arange(0, 1.1, 0.2))\n",
    "plt.ylim(0, 1)\n",
    "plt.title('PERMA', fontsize=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emorec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c67f0142598d7b06f34a2acb15d98e76cb3b1e66b7b86f21a91e42da5de8188"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

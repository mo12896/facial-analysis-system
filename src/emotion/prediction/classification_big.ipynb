{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, SelectPercentile, SelectFpr, SelectFdr, SelectFwe\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from scipy.stats import skew\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "sys.path.append('../../../')\n",
    "\n",
    "from src.emotion.prediction.aggregates.train_classifier import HyperparaSearchClassifier\n",
    "from src.emotion.prediction.aggregates.classifier import CLASSIFIER\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# set default color cycle\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=plt.cm.tab10.colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('/home/moritz/Workspace/masterthesis/data/features_dataset_big.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_csv('/home/moritz/Workspace/masterthesis/data/perma_scores_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 9351)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(features, targets, on=[\"E-Mail-Adresse\", \"Day\"])\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103, 9348)\n"
     ]
    }
   ],
   "source": [
    "# Handle Missing Values\n",
    "\n",
    "df.dropna(axis=1, how='any', inplace=True)\n",
    "#df = dataset.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103, 8757)\n",
      "(103, 8757)\n"
     ]
    }
   ],
   "source": [
    "# Detect outliers\n",
    "# find columns where all values are the same\n",
    "cols_to_drop = [col for col in df.columns if df[col].nunique() == 1]\n",
    "# drop the columns\n",
    "df = df.drop(cols_to_drop, axis=1)\n",
    "print(df.shape)\n",
    "\n",
    "# drop columns where all values are only 0 or 1\n",
    "df = df.loc[:, ~(df.isin([0, 1]).all() & ~df.isin([0, 1]).any())]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load X and Y\n",
    "# Store the PERMA values in Y\n",
    "Y = df[['P', 'E', 'R', 'M', 'A']]\n",
    "\n",
    "# Store the other columns in X\n",
    "X = df.drop(columns=['ClassID', 'E-Mail-Adresse', 'Day', 'First Name', 'Last Name/Surname', 'P', 'E', 'R', 'M', 'A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n"
     ]
    }
   ],
   "source": [
    "# Scale Features\n",
    "# Create a MinMaxScaler object\n",
    "minmax_scaler = MinMaxScaler()\n",
    "gaussian_scaler = StandardScaler()\n",
    "non_gaussian_scaler = RobustScaler()\n",
    "\n",
    "# Calculate skewness of each feature\n",
    "skewness = skew(X, axis=0)\n",
    "\n",
    "# Create empty list to store scaled features\n",
    "scaled_features = []\n",
    "\n",
    "# Scale features using appropriate scaler\n",
    "for feat, s in zip(X.columns, skewness):\n",
    "    if abs(s) < 0.5:\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = RobustScaler()\n",
    "    scaled_feat = scaler.fit_transform(X[feat].values.reshape(-1, 1)).flatten()\n",
    "    scaled_features.append(pd.Series(scaled_feat, name=feat))\n",
    "\n",
    "# Concatenate scaled features into new DataFrame\n",
    "scaled_X = pd.concat(scaled_features, axis=1)\n",
    "\n",
    "\n",
    "# Fit the scaler to the dataframe and transform the dataframe\n",
    "#scaled_Y = pd.DataFrame(minmax_scaler.fit_transform(Y), columns=['P', 'E', 'R', 'M', 'A'])\n",
    "scaled_Y = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop constant features (zero variance)\n",
    "# var_x = scaled_X.var()\n",
    "# cols_to_drop = [col for col in scaled_X.columns if var_x[col] == 0]\n",
    "# scaled_X = scaled_X.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "def assign_class_based_on_median(df):\n",
    "    # Select only numerical columns\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # Calculate the median value of all numerical values in the DataFrame\n",
    "    median_value = df[numeric_columns].stack().median()\n",
    "\n",
    "    # Define a helper function to classify values based on the median value\n",
    "    def classify(value):\n",
    "        if value < median_value:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    # Apply the helper function to each numerical value in the DataFrame\n",
    "    for col in numeric_columns:\n",
    "        df[col] = df[col].apply(classify)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Assign each numerical value to either class 0 or class 1\n",
    "scaled_Y = assign_class_based_on_median(scaled_Y)\n",
    "#print(scaled_Y[\"E\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the DataFrames to NumPy arrays\n",
    "# X_np = scaled_X.to_numpy()\n",
    "# Y_np = scaled_Y.to_numpy()\n",
    "\n",
    "# # Perform the train/test split\n",
    "# X_train_np, y_train_np, X_test_np, y_test_np = iterative_train_test_split(X_np, Y_np, test_size=0.2)\n",
    "\n",
    "# # Convert the NumPy arrays back to DataFrames\n",
    "# X_train = pd.DataFrame(X_train_np, columns=scaled_X.columns)\n",
    "# Y_train = pd.DataFrame(y_train_np, columns=scaled_Y.columns)\n",
    "# X_test = pd.DataFrame(X_test_np, columns=scaled_X.columns)\n",
    "# Y_test = pd.DataFrame(y_test_np, columns=scaled_Y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(scaled_X, scaled_Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set class distribution:\n",
      "P: 0.4878048780487805\n",
      "E: 0.4634146341463415\n",
      "R: 0.5853658536585366\n",
      "M: 0.5365853658536586\n",
      "A: 0.5975609756097561\n",
      "\n",
      "Test set class distribution:\n",
      "P: 0.47619047619047616\n",
      "E: 0.5238095238095238\n",
      "R: 0.5238095238095238\n",
      "M: 0.47619047619047616\n",
      "A: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "# Check the class distributions in the train and test sets\n",
    "print(\"Train set class distribution:\")\n",
    "for col in scaled_Y.columns:\n",
    "    print(f\"{col}: {Y_train[col].mean()}\")\n",
    "\n",
    "print(\"\\nTest set class distribution:\")\n",
    "for col in scaled_Y.columns:\n",
    "    print(f\"{col}: {Y_test[col].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_selected_voting_features_combined(Y, X, filter_methods, wrapper_methods, k):\n",
    "    \n",
    "    feature_counts = np.zeros(X.shape[1])\n",
    "\n",
    "    # Apply filter methods\n",
    "    for _, method in filter_methods.items():\n",
    "        method.fit(X, Y)\n",
    "        selected_i = method.get_support(indices=True)\n",
    "        print(len(selected_i))\n",
    "        \n",
    "        # Increment the count for each selected feature\n",
    "        for index in selected_i:\n",
    "            feature_counts[index] += 1\n",
    "\n",
    "    # Apply wrapper methods\n",
    "    for _, method in wrapper_methods.items():\n",
    "        method.fit(X, Y)\n",
    "        selected_i = method.get_support(indices=True)\n",
    "        \n",
    "        # Increment the count for each selected feature\n",
    "        for index in selected_i:\n",
    "            feature_counts[index] += 1\n",
    "    \n",
    "    # Get the indices of the top k features with the most counts\n",
    "    #selected_features = np.argsort(feature_counts)[-k:]\n",
    "    selected_features = np.where(feature_counts >= k)[0]\n",
    "    print(np.sort(feature_counts))\n",
    "    \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "44\n",
      "24\n",
      "0\n",
      "[0. 0. 0. ... 3. 3. 3.]\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n",
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n",
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n",
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "44\n",
      "28\n",
      "1\n",
      "[0. 0. 0. ... 3. 3. 4.]\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n",
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n",
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n",
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "44\n",
      "301\n",
      "17\n",
      "[0. 0. 0. ... 4. 4. 4.]\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n",
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n",
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n",
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "44\n",
      "36\n",
      "2\n",
      "[0. 0. 0. ... 3. 4. 4.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n",
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n",
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n",
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "44\n",
      "18\n",
      "1\n",
      "[0. 0. 0. ... 3. 4. 5.]\n",
      "P : [1698 1734 1958 3455 4219 4962 6797 6816]\n",
      "E : [ 291  604 1749 3797 5241 5706 6700 7881]\n",
      "R : [  41   80  626  627 1772 1952 1958 1974 2807 2987 4866 5095 5582 5825\n",
      " 5850 6553 7895]\n",
      "M : [2276 2277 2281 2285 2743 3243 3895 4453 4457]\n",
      "A : [2575 3912 4039 4238 4585 4763 6883 6986]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n",
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n",
      "Features [2165 5065 5790 5791 7249 7250] are constant.\n",
      "invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Create a dictionary for each PERMA pillar\n",
    "perma_dict = {\n",
    "    \"P\": None,\n",
    "    \"E\": None,\n",
    "    \"R\": None,\n",
    "    \"M\": None,\n",
    "    \"A\": None,\n",
    "}\n",
    "\n",
    "filter_methods = {\n",
    "    #'chi2': SelectKBest(chi2, k=8),\n",
    "    'f_classif': SelectKBest(f_classif, k=8),\n",
    "    'mutual_info_classif': SelectKBest(mutual_info_classif, k=8),\n",
    "    #'variance_threshold': VarianceThreshold(threshold=2.0),\n",
    "    'select_percentile': SelectPercentile(f_classif, percentile=0.5),\n",
    "    'select_fpr': SelectFpr(f_classif, alpha=0.005),\n",
    "    #'select_fdr': SelectFdr(f_classif, alpha=0.915),\n",
    "    'select_fwe': SelectFwe(f_classif, alpha=1.0),\n",
    "    #'rfe_classification': RFEC(LogisticRegression(), n_features_to_select=8),\n",
    "}\n",
    "\n",
    "wrapper_methods = {\n",
    "    #'sfs_classification': SequentialFeatureSelector(LogisticRegression(), n_features_to_select=8, direction='forward', n_jobs=-1),\n",
    "    #'sbs_classification': SequentialFeatureSelector(LogisticRegression(), n_features_to_select=8, direction='backward', n_jobs=-1),\n",
    "}\n",
    "\n",
    "# Iterate over each PERMA dimension in Y\n",
    "for i, perma_dim in enumerate(Y.columns):\n",
    "    # Get the selected features for the current PERMA dimension\n",
    "    selected_features = get_selected_voting_features_combined(Y_train.iloc[:, i], X_train, filter_methods, wrapper_methods, k=3)\n",
    "    # Save the selected features in the corresponding dictionary for the current PERMA pillar\n",
    "    perma_dict[perma_dim] = selected_features\n",
    "\n",
    "# Print the selected features for each PERMA dimension in each PERMA pillar\n",
    "for pillar in perma_dict:\n",
    "    print(pillar, \":\", perma_dict[pillar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P : ['Dominance__fft_coefficient__attr_\"real\"__coeff_52', 'Happy__fft_coefficient__attr_\"real\"__coeff_17', 'Neutral__fft_coefficient__attr_\"angle\"__coeff_80', 'Happy__fft_coefficient__attr_\"abs\"__coeff_78', 'Happy__fft_coefficient__attr_\"real\"__coeff_53', 'Dominance__fft_coefficient__attr_\"real\"__coeff_33', 'Fear__fft_coefficient__attr_\"angle\"__coeff_61', 'Surprise__fft_coefficient__attr_\"angle\"__coeff_23']\n",
      "E : ['Angry__fft_coefficient__attr_\"real\"__coeff_63', 'Brightness__fft_coefficient__attr_\"angle\"__coeff_87', 'Valence__fft_aggregated__aggtype_\"skew\"', 'Dominance__ar_coefficient__coeff_7__k_10', 'Fear__ar_coefficient__coeff_8__k_10', 'Happy__fft_coefficient__attr_\"real\"__coeff_68', 'Valence__ar_coefficient__coeff_7__k_10', 'Angry__fft_coefficient__attr_\"angle\"__coeff_78']\n",
      "R : ['Happy__fft_coefficient__attr_\"abs\"__coeff_72', 'Arousal__absolute_sum_of_changes', 'Neutral__fft_coefficient__attr_\"abs\"__coeff_83', 'Happy__fft_coefficient__attr_\"abs\"__coeff_78', 'Valence__absolute_sum_of_changes', 'Angry__cid_ce__normalize_True', 'Brightness__fft_aggregated__aggtype_\"variance\"', 'Surprise__number_peaks__n_3', 'Happy__fft_coefficient__attr_\"real\"__coeff_91', 'Valence__fft_coefficient__attr_\"abs\"__coeff_78', 'Angry__number_peaks__n_1', 'Angry__fft_aggregated__aggtype_\"centroid\"', 'Angry__fft_aggregated__aggtype_\"variance\"', 'Happy__fft_coefficient__attr_\"abs\"__coeff_94', 'Sad__fft_aggregated__aggtype_\"variance\"', 'Dominance__absolute_sum_of_changes', 'Arousal__cid_ce__normalize_False']\n",
      "M : ['Sad__cwt_coefficients__coeff_0__w_10__widths_(2, 5, 10, 20)', 'Sad__cwt_coefficients__coeff_0__w_20__widths_(2, 5, 10, 20)', 'Neutral__cwt_coefficients__coeff_0__w_20__widths_(2, 5, 10, 20)', 'Sad__cwt_coefficients__coeff_1__w_20__widths_(2, 5, 10, 20)', 'Neutral__cwt_coefficients__coeff_1__w_20__widths_(2, 5, 10, 20)', 'Surprise__fft_coefficient__attr_\"imag\"__coeff_10', 'Sad__cwt_coefficients__coeff_2__w_20__widths_(2, 5, 10, 20)', 'Fear__fft_coefficient__attr_\"real\"__coeff_35', 'Sad__fft_coefficient__attr_\"angle\"__coeff_37']\n",
      "A : ['Dominance__fft_coefficient__attr_\"imag\"__coeff_20', 'Fear__fft_coefficient__attr_\"imag\"__coeff_80', 'Fear__fft_coefficient__attr_\"real\"__coeff_52', 'Neutral__fft_coefficient__attr_\"real\"__coeff_1', 'Dominance__fft_coefficient__attr_\"abs\"__coeff_23', 'Fear__fft_coefficient__attr_\"angle\"__coeff_80', 'Sad__fft_coefficient__attr_\"imag\"__coeff_68', 'Neutral__fft_coefficient__attr_\"imag\"__coeff_80']\n"
     ]
    }
   ],
   "source": [
    "feature_sets = {\n",
    "    \"P\": {\"train\": None, \"test\": None},\n",
    "    \"E\": {\"train\": None, \"test\": None},\n",
    "    \"R\": {\"train\": None, \"test\": None},\n",
    "    \"M\": {\"train\": None, \"test\": None},\n",
    "    \"A\": {\"train\": None, \"test\": None}\n",
    "}\n",
    "\n",
    "for perma_dim, selected_features in perma_dict.items():\n",
    "    # Select the corresponding columns of X_filtered\n",
    "    X_train_final = X_train.iloc[:, list(set(selected_features))]\n",
    "    X_test_final = X_test.iloc[:, list(set(selected_features))]\n",
    "    # Add the selected features for the current PERMA dimension to the feature sets dictionary\n",
    "    feature_sets[perma_dim][\"train\"] = X_train_final\n",
    "    feature_sets[perma_dim][\"test\"] = X_test_final\n",
    "    # Print the names of the selected features\n",
    "    column_names = X_train_final.columns.tolist()\n",
    "    print(perma_dim, \":\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR_CLASSIFIER = Path(\"/home/moritz/Workspace/masterthesis/model/custom_models/classifier/small\")\n",
    "\n",
    "eval_metric = \"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "CLASSIFIER = [\n",
    "    {\n",
    "        \"name\": \"MLPClassifier\",\n",
    "        \"model\": MLPClassifier(max_iter=2000),\n",
    "        \"params\": {\n",
    "            \"hidden_layer_sizes\": [(64, 32), (128, 64)],\n",
    "            \"alpha\": [0.001, 0.01, 0.1],\n",
    "            \"learning_rate_init\": [0.001, 0.01, 0.1],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"KNeighborsClassifier\",\n",
    "        \"model\": KNeighborsClassifier(),\n",
    "        \"params\": {\"n_neighbors\": [1, 5, 10, 15, 20]},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DecisionTreeClassifier\",\n",
    "        \"model\": DecisionTreeClassifier(),\n",
    "        \"params\": {\"max_depth\": range(2, 11)},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"RandomForestClassifier\",\n",
    "        \"model\": RandomForestClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200, 400],\n",
    "            \"max_depth\": range(2, 6),\n",
    "            \"class_weight\": [\"balanced\", \"balanced_subsample\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ExtraTreesClassifier\",\n",
    "        \"model\": ExtraTreesClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200, 400],\n",
    "            \"max_depth\": range(2, 6),\n",
    "            \"class_weight\": [\"balanced\", \"balanced_subsample\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GradientBoostingClassifier\",\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200, 400],\n",
    "            \"max_depth\": range(2, 6),\n",
    "            \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"AdaBoostClassifier\",\n",
    "        \"model\": AdaBoostClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200, 400],\n",
    "            \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SVC\",\n",
    "        \"model\": SVC(),\n",
    "        \"params\": {\n",
    "            \"kernel\": [\"linear\", \"rbf\"],\n",
    "            \"C\": [0.1, 1.0, 10.0],\n",
    "            \"shrinking\": [True, False],\n",
    "            \"class_weight\": [\"balanced\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"LogisticRegression\",\n",
    "        \"model\": LogisticRegression(max_iter=2000),\n",
    "        \"params\": {\n",
    "            \"C\": [0.001, 0.01, 0.1, 1.0],\n",
    "            \"penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "            \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n",
    "            \"class_weight\": [\"balanced\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"RidgeClassifier\",\n",
    "        \"model\": RidgeClassifier(),\n",
    "        \"params\": {\n",
    "            \"alpha\": [0.001, 0.01, 0.1, 1.0],\n",
    "            \"class_weight\": [\"balanced\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GaussianNB\",\n",
    "        \"model\": GaussianNB(),\n",
    "        \"params\": {\"var_smoothing\": [1e-9, 1e-8, 1e-7]},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CatBoostClassifier\",\n",
    "        \"model\": CatBoostClassifier(verbose=False),\n",
    "        \"params\": {\n",
    "            \"iterations\": [50, 100, 200, 400],\n",
    "            \"depth\": range(2, 6),\n",
    "            \"learning_rate\": [0.001, 0.01, 0.1, 1.0],\n",
    "            \"auto_class_weights\": [\"Balanced\", \"SqrtBalanced\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"XGBClassifier\",\n",
    "        \"model\": XGBClassifier(),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200, 400],\n",
    "            \"max_depth\": range(2, 6),\n",
    "            \"learning_rate\": [0.001, 0.01, 0.1],\n",
    "            \"scale_pos_weight\": [99],\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "RandomForestClassifier\n",
      "ExtraTreesClassifier\n",
      "AdaBoostClassifier\n",
      "SVC\n",
      "LogisticRegression\n",
      "RidgeClassifier\n",
      "GaussianNB\n",
      "CatBoostClassifier\n",
      "XGBClassifier\n"
     ]
    }
   ],
   "source": [
    "classifier_to_drop = [\"MLPClassifier\", \"KNeighborsClassifier\", \"GradientBoostingClassifier\"]\n",
    "\n",
    "for name in classifier_to_drop:\n",
    "    for i in range(len(CLASSIFIER)):\n",
    "        if CLASSIFIER[i][\"name\"] == name:\n",
    "            del CLASSIFIER[i]\n",
    "            break\n",
    "        \n",
    "for classifier in CLASSIFIER:\n",
    "    print(classifier[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERMA dimension: P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "160 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/joblib/parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/joblib/parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/joblib/parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/joblib/parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/moritz/anaconda3/envs/emorec/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.51323529        nan 0.45147059 0.84264706\n",
      " 0.84264706 0.83014706 0.84264706 0.84264706        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.51323529\n",
      "        nan 0.51323529 0.83088235 0.83088235 0.83014706 0.83088235\n",
      " 0.83088235        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.64705882        nan 0.63602941 0.83088235\n",
      " 0.83088235 0.84264706 0.83088235 0.83088235        nan        nan\n",
      "        nan        nan        nan        nan        nan 0.86617647\n",
      "        nan 0.83014706 0.85441176 0.85441176 0.85441176 0.85441176\n",
      " 0.85441176        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results_classifier_dict = {}\n",
    "\n",
    "for perma_dim, X_final in feature_sets.items():\n",
    "    print(f\"PERMA dimension: {perma_dim}\")\n",
    "    # Run the hyperparameter search\n",
    "    models_path = SAVE_DIR_CLASSIFIER / perma_dim\n",
    "    X_train = X_final[\"train\"]\n",
    "    search = HyperparaSearchClassifier(models=CLASSIFIER, metrics=[\"accuracy\"], models_path = models_path, n_folds=5, n_jobs=-1, mode=\"uni\")  \n",
    "    results = search.run(X_train, Y_train[perma_dim], save=True)\n",
    "    results_classifier_dict[perma_dim] = results\n",
    "    # Print the model with the lowest score\n",
    "    best_model = min(\n",
    "        [\n",
    "            min(\n",
    "                sublist,\n",
    "                key=lambda x: x[\"score\"]\n",
    "                if x[\"metric\"] == eval_metric\n",
    "                else float(\"inf\"),\n",
    "            )\n",
    "            for sublist in results\n",
    "        ],\n",
    "        key=lambda x: x[\"score\"],\n",
    "    )\n",
    "    print(perma_dim)\n",
    "    print(f\"Best model: {best_model['name']}\")\n",
    "    print(f\"Best params: {best_model['params']}\")\n",
    "    print(f\"Best Score: {best_model['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = {}\n",
    "\n",
    "for perma_dim, results in results_classifier_dict.items():\n",
    "    best_model = min(\n",
    "        [\n",
    "            min(\n",
    "                sublist,\n",
    "                key=lambda x: x[\"score\"]\n",
    "                if x[\"metric\"] == eval_metric\n",
    "                else float(\"inf\"),\n",
    "            )\n",
    "            for sublist in results\n",
    "        ],\n",
    "        key=lambda x: x[\"score\"],\n",
    "    )\n",
    "    # print(perma_dim)\n",
    "    # print(f\"Best model: {best_model['name']}\")\n",
    "    # print(f\"Best params: {best_model['params']}\")\n",
    "    # print(f\"Best Score: {best_model['score']}\")\n",
    "    best_models[perma_dim] = best_model['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model(file_path: Path):\n",
    "\n",
    "    models = {}\n",
    "\n",
    "    if file_path.is_file():\n",
    "        model = joblib.load(file_path)\n",
    "\n",
    "        models[str(file_path.stem)] = model\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perma_models = {}\n",
    "\n",
    "for perma_dim in [\"P\", \"E\", \"R\", \"M\", \"A\"]:\n",
    "    models_path = SAVE_DIR_CLASSIFIER / perma_dim / (best_models[perma_dim] + \".joblib\")\n",
    "    models = load_best_model(models_path)\n",
    "    perma_models[perma_dim] = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(models, X, y):\n",
    "    # Generate and return a dictionary of balanced accuracy scores and prediction arrays for each model\n",
    "    results = {}\n",
    "    for model_name, mae_grid_search in models.items():\n",
    "        # Fit the model\n",
    "        model = mae_grid_search[0].best_estimator_\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X)\n",
    "        # Calculate balanced accuracy score\n",
    "        balanced_accuracy = balanced_accuracy_score(y, y_pred)\n",
    "        results[model_name] = {\"accuracy\": balanced_accuracy, \"y_pred\": y_pred}\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shap_values(models, X):\n",
    "    for model_name, mae_grid_search in models.items():\n",
    "        model = mae_grid_search[0].best_estimator_\n",
    "        print(model_name)\n",
    "\n",
    "        if model_name in [\"LogisticRegression\", \"RidgeClassifier\"]:\n",
    "            explainer = shap.LinearExplainer(model, X, feature_names=X.columns)\n",
    "            shap_values = explainer(X)\n",
    "            shap.summary_plot(shap_values, X)\n",
    "        elif model_name in [\"AdaBoostClassifier\", \"GaussianNB\", \"DecisionTreeClassifier\"]:\n",
    "            explainer = shap.SamplingExplainer(model.predict, X)\n",
    "            shap_values = explainer(X)\n",
    "            shap.summary_plot(shap_values, X)\n",
    "        elif model_name in [\"SVC\"]:\n",
    "            explainer = shap.KernelExplainer(model.predict, X)\n",
    "            shap_values = explainer(X)\n",
    "            shap.summary_plot(shap_values, X)\n",
    "        elif model_name in [\"CatBoostClassifier\"]:\n",
    "            explainer = shap.Explainer(model, feature_names=X.columns)\n",
    "            shap_values = explainer(X)\n",
    "            shap.summary_plot(shap_values, X)\n",
    "        else:\n",
    "            explainer = shap.TreeExplainer(model, feature_names=X.columns)\n",
    "            shap_values = explainer(X)\n",
    "            shap.summary_plot(shap_values, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim, models in perma_models.items():\n",
    "    print(dim)\n",
    "    X_test = feature_sets[dim][\"train\"]\n",
    "    generate_shap_values(models, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perma_results = {}\n",
    "\n",
    "for dim, models in perma_models.items():\n",
    "    X_test = feature_sets[dim][\"test\"]\n",
    "    results = generate_predictions(models, X_test, Y_test[dim])\n",
    "    perma_results[dim] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = {}\n",
    "\n",
    "for dim, results in perma_results.items():\n",
    "    curr_mae = float(\"inf\")\n",
    "    for model in results:\n",
    "        if results[model][\"accuracy\"] < curr_mae:\n",
    "            curr_mae = results[model][\"accuracy\"]\n",
    "            best_results[dim] = {\"model\": model, \"accuracy\": results[model][\"accuracy\"], \"preds\": results[model][\"y_pred\"]}\n",
    "            \n",
    "print(best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline:\n",
    "accuracy_baseline = [0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "\n",
    "print(f\"Baseline accuracy for each dimension: {accuracy_baseline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best model for each PERMA dimension\n",
    "best_accuracy_values = [v['accuracy'] for v in best_results.values()]\n",
    "\n",
    "# Define the x-axis labels and the bar width\n",
    "perma_dimensions = ['P', 'E', 'R', 'M', 'A']\n",
    "bar_width = 0.35\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots()\n",
    "palette = sns.color_palette(\"muted\")\n",
    "ax.set_prop_cycle(color=palette)\n",
    "ax.bar(np.arange(len(perma_dimensions)), accuracy_baseline, width=bar_width, label='Baseline')\n",
    "ax.bar(np.arange(len(perma_dimensions))+bar_width, best_accuracy_values, width=bar_width, label='Best Models')\n",
    "\n",
    "# Set the x-axis ticks and labels\n",
    "ax.set_xticks(np.arange(len(perma_dimensions))+bar_width/2)\n",
    "ax.set_xticklabels(perma_dimensions)\n",
    "ax.set_xlabel('PERMA Dimensions')\n",
    "\n",
    "# Set the y-axis label and limits\n",
    "ax.set_ylabel('Accuracy scores')\n",
    "ax.set_ylim([0, max(np.max(accuracy_baseline), np.max(best_accuracy_values))*1.1])\n",
    "\n",
    "# Add the model names and MAE values above each bar\n",
    "for i, v in enumerate(accuracy_baseline):\n",
    "    ax.text(i, 0.005, f\"Accuracy: {v:.3f}\", rotation=90, ha='center', va='bottom', fontsize=10, color='white')\n",
    "    ax.text(i+bar_width, 0.005, f\"MAE: {best_mae_values[i]:.3f} ({best_results[perma_dimensions[i]]['model']})\", rotation=90, ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "# Add a legend and title\n",
    "ax.legend()\n",
    "ax.set_title('Accuracy-scores on PERMA dimensions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emorec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c67f0142598d7b06f34a2acb15d98e76cb3b1e66b7b86f21a91e42da5de8188"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
